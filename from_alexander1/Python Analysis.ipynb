{
 "metadata": {
  "name": "",
  "signature": "sha256:9ab6ea35e9b023eba8d0117789848b62f7e57323ed0ae28cfb2d6813c8a7c55c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to spark\n",
      "from pyspark import SparkContext\n",
      "\n",
      "# Configure the necessary Spark environment\n",
      "import os\n",
      "os.environ['SPARK_HOME'] = '/opt/spark/'\n",
      "\n",
      "# And Python path\n",
      "import sys\n",
      "sys.path.insert(0, '/opt/spark/python')\n",
      "\n",
      "# Detect the PySpark URL\n",
      "CLUSTER_URL = 'spark://alexander1:7077'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc = SparkContext( CLUSTER_URL, 'pyspark')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import subprocess\n",
      "\n",
      "master_list = []\n",
      "\n",
      "# Get a list of all the folders\n",
      "folders_list = [\"Dog_1\",\"Dog_2\",\"Dog_3\",\"Dog_4\",\"Dog_5\",\"Patient_1\",\"Patient_2\"]\n",
      "\n",
      "for eachfolder in folders_list:\n",
      "    \n",
      "    new_list = []\n",
      "    \n",
      "    # Get a list of all the files to process\n",
      "    p = subprocess.Popen(\"hadoop fs -ls hdfs://alexander1:9000/user/root/seizure-data/\"+eachfolder+\"/\", \n",
      "                         shell='T', stdout=subprocess.PIPE)\n",
      "    #p.wait()\n",
      "    # check return code\n",
      "    #ls_lines = p.stdout.readlines()\n",
      "\n",
      "    # take just the last \n",
      "    file_list = p.stdout.readlines()\n",
      "    \n",
      "    # take just the filename\n",
      "    for each in file_list:\n",
      "        splitted = each.split(\" \")\n",
      "        splitted = splitted[len(splitted)-1]\n",
      "        splitted = splitted[:-1]\n",
      "        new_list.append( splitted )\n",
      "\n",
      "    new_list = new_list[5:]\n",
      "    \n",
      "    master_list = new_list + master_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_1/Dog_1_interictal_segment_0010.mat']\n",
        "['hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_2/Dog_2_interictal_segment_0010.mat']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_3/Dog_3_interictal_segment_0010.mat']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_4/Dog_4_interictal_segment_0010.mat']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Dog_5/Dog_5_interictal_segment_0010.mat']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_1/Patient_1_interictal_segment_0010.mat']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0002.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0003.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0004.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0005.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0006.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0007.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0008.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0009.mat', 'hdfs://alexander1:9000/user/root/seizure-data/Patient_2/Patient_2_interictal_segment_0010.mat']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "all_files = master_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "dir_name = \"seizure-data-preprocessed\"\n",
      "\n",
      "# remove directory if exists\n",
      "p = subprocess.Popen('hdfs dfs -rmr hdfs://alexander1:9000/user/root/'+dir_name, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
      "retval = p.wait()\n",
      "\n",
      "# create an empty directory on hdfs\n",
      "p = subprocess.Popen('hdfs dfs -mkdir hdfs://alexander1:9000/user/root/'+dir_name, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
      "retval = p.wait()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process(filename):\n",
      "    import subprocess\n",
      "    import os\n",
      "    \n",
      "    # retreive the file from hdfs\n",
      "    if os.path.exists(\"/hdfs/tmp.mat\"):\n",
      "        p = subprocess.Popen('rm /hdfs/tmp.mat',shell=\"TRUE\")\n",
      "        retval = p.wait()\n",
      "    \n",
      "    p = subprocess.Popen('hdfs dfs -get '+filename+' /hdfs/tmp.mat', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
      "    retval = p.wait()\n",
      "\n",
      "    # convert from mat\n",
      "    import scipy.io\n",
      "    mat = scipy.io.loadmat( \"/hdfs/tmp.mat\" )\n",
      "    \n",
      "    list_keys = mat.keys()\n",
      "    \n",
      "    for item in list_keys:\n",
      "        if item[0:1] is not \"_\":\n",
      "            key_name = item\n",
      "    \n",
      "    \n",
      "    # strip key factors\n",
      "    filename_split = filename.split(\"/\")\n",
      "    file = filename_split[len(filename_split)-1]\n",
      "    file_split = file.split(\"_\")\n",
      "    patient_type = file_split[0]\n",
      "    patient_name = patient_type + \"_\" + file_split[1]\n",
      "    segment_type = file_split[2]\n",
      "    segment = str(int(file_split[len(file_split)-1][:-4]))\n",
      "    data_length_sec = str(mat[key_name]['data_length_sec'][0][0][0][0])\n",
      "    sampling_frequency = str(mat[key_name]['sampling_frequency'][0][0][0][0])\n",
      "    n_channels = str(len(mat[key_name]['channels'][0][0][0]))\n",
      "    # look for sequence, use it if it exists\n",
      "    sequence = str(\"0\")\n",
      "    for eachtype in mat[key_name].dtype.fields.keys():\n",
      "        if eachtype=='sequence':\n",
      "            sequence = str(mat[key_name]['sequence'][0][0][0][0])\n",
      "        \n",
      "    \n",
      "    # save off temp results to hd\n",
      "    with open(\"/hdfs/tmp.csv\", \"w\") as text_file:\n",
      "        text_file.write(file\n",
      "                        +\",\"+patient_type\n",
      "                        +\",\"+patient_name\n",
      "                        +\",\"+segment_type\n",
      "                        +\",\"+segment_type\n",
      "                        +\",\"+segment\n",
      "                        +\",\"+data_length_sec\n",
      "                        +\",\"+sampling_frequency\n",
      "                        +\",\"+n_channels\n",
      "                        +\",\"+sequence)\n",
      "    \n",
      "    # upload results to hdfs from hd\n",
      "    p = subprocess.Popen('hdfs dfs -put /hdfs/tmp.csv hdfs://alexander1:9000/user/root/'+dir_name+'/'+filename.split(\"/\")[7], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
      "    retval = p.wait()\n",
      "\n",
      "#seqrdd = sc.parallelize(all_files.keys().take(3),1).flatMap(process)\n",
      "#seqrdd = all_files.keys().flatMap(process)\n",
      "#sc.parallelize(all_files.keys().take(1),1).foreach(process)\n",
      "sc.parallelize(all_files,3000).foreach(process)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o427.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 4 times, most recent failure: Lost task 6.3 in stage 0.0 (TID 31, alexander4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/pyspark/worker.py\", line 79, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/pyspark/serializers.py\", line 196, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/pyspark/serializers.py\", line 127, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/pyspark/serializers.py\", line 185, in _batched\n    for item in iterator:\n  File \"/opt/spark/python/pyspark/rdd.py\", line 702, in processPartition\n    f(x)\n  File \"<ipython-input-136-18b60d0833dd>\", line 15, in process\n  File \"/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio.py\", line 132, in loadmat\n    matfile_dict = MR.get_variables(variable_names)\n  File \"/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5.py\", line 292, in get_variables\n    res = self.read_var_array(hdr, process)\n  File \"/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5.py\", line 252, in read_var_array\n    return self._matrix_reader.array_from_header(header, process)\n  File \"mio5_utils.pyx\", line 625, in scipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5993)\n  File \"mio5_utils.pyx\", line 673, in scipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5585)\n  File \"mio5_utils.pyx\", line 922, in scipy.io.matlab.mio5_utils.VarReader5.read_struct (scipy/io/matlab/mio5_utils.c:8494)\n  File \"mio5_utils.pyx\", line 623, in scipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix (scipy/io/matlab/mio5_utils.c:5184)\n  File \"mio5_utils.pyx\", line 655, in scipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5374)\n  File \"mio5_utils.pyx\", line 728, in scipy.io.matlab.mio5_utils.VarReader5.read_real_complex (scipy/io/matlab/mio5_utils.c:6413)\n  File \"mio5_utils.pyx\", line 418, in scipy.io.matlab.mio5_utils.VarReader5.read_numeric (scipy/io/matlab/mio5_utils.c:3882)\n  File \"mio5_utils.pyx\", line 354, in scipy.io.matlab.mio5_utils.VarReader5.read_element (scipy/io/matlab/mio5_utils.c:3604)\n  File \"streams.pyx\", line 326, in scipy.io.matlab.streams.FileStream.read_string (scipy/io/matlab/streams.c:4445)\nIOError: could not read bytes\n\n        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)\n        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)\n        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)\n        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n        org.apache.spark.scheduler.Task.run(Task.scala:54)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        java.lang.Thread.run(Unknown Source)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-136-18b60d0833dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#seqrdd = all_files.keys().flatMap(process)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m#sc.parallelize(all_files.keys().take(1),1).foreach(process)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/opt/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mforeach\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    702\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessPartition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Force evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforeachPartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \"\"\"\n\u001b[0;32m    722\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_JavaStackTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m             \u001b[0mbytesInJava\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o427.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 4 times, most recent failure: Lost task 6.3 in stage 0.0 (TID 31, alexander4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/pyspark/worker.py\", line 79, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/pyspark/serializers.py\", line 196, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/pyspark/serializers.py\", line 127, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/pyspark/serializers.py\", line 185, in _batched\n    for item in iterator:\n  File \"/opt/spark/python/pyspark/rdd.py\", line 702, in processPartition\n    f(x)\n  File \"<ipython-input-136-18b60d0833dd>\", line 15, in process\n  File \"/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio.py\", line 132, in loadmat\n    matfile_dict = MR.get_variables(variable_names)\n  File \"/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5.py\", line 292, in get_variables\n    res = self.read_var_array(hdr, process)\n  File \"/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5.py\", line 252, in read_var_array\n    return self._matrix_reader.array_from_header(header, process)\n  File \"mio5_utils.pyx\", line 625, in scipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5993)\n  File \"mio5_utils.pyx\", line 673, in scipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5585)\n  File \"mio5_utils.pyx\", line 922, in scipy.io.matlab.mio5_utils.VarReader5.read_struct (scipy/io/matlab/mio5_utils.c:8494)\n  File \"mio5_utils.pyx\", line 623, in scipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix (scipy/io/matlab/mio5_utils.c:5184)\n  File \"mio5_utils.pyx\", line 655, in scipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5374)\n  File \"mio5_utils.pyx\", line 728, in scipy.io.matlab.mio5_utils.VarReader5.read_real_complex (scipy/io/matlab/mio5_utils.c:6413)\n  File \"mio5_utils.pyx\", line 418, in scipy.io.matlab.mio5_utils.VarReader5.read_numeric (scipy/io/matlab/mio5_utils.c:3882)\n  File \"mio5_utils.pyx\", line 354, in scipy.io.matlab.mio5_utils.VarReader5.read_element (scipy/io/matlab/mio5_utils.c:3604)\n  File \"streams.pyx\", line 326, in scipy.io.matlab.streams.FileStream.read_string (scipy/io/matlab/streams.c:4445)\nIOError: could not read bytes\n\n        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:124)\n        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:154)\n        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)\n        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n        org.apache.spark.scheduler.Task.run(Task.scala:54)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        java.lang.Thread.run(Unknown Source)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test one at a time\n",
      "import scipy.io\n",
      "mat = scipy.io.loadmat( binary_mat )\n",
      "\n",
      "sequence = str(\"\")\n",
      "list_keys = mat.keys()\n",
      "for item in list_keys:\n",
      "    if item[0:1] is not \"_\":\n",
      "        key_name = item\n",
      "\n",
      "sequence = str(\"\")\n",
      "for eachtype in mat[key_name].dtype.fields.keys():\n",
      "    if eachtype=='sequence':\n",
      "        sequence = str(mat[key_name]['sequence'][0][0][0][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "could not read bytes",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-98-a26573573e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test one at a time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"/hdfs/tmp.mat\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mMR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mmdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatfile_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5.pyc\u001b[0m in \u001b[0;36mget_variables\u001b[1;34m(self, variable_names)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 warnings.warn(\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5.pyc\u001b[0m in \u001b[0;36mread_var_array\u001b[1;34m(self, header, process)\u001b[0m\n\u001b[0;32m    250\u001b[0m            \u001b[1;33m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         '''\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5993)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5585)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct (scipy/io/matlab/mio5_utils.c:8494)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix (scipy/io/matlab/mio5_utils.c:5184)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5374)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_real_complex (scipy/io/matlab/mio5_utils.c:6413)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_numeric (scipy/io/matlab/mio5_utils.c:3882)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_element (scipy/io/matlab/mio5_utils.c:3604)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/scipy/io/matlab/streams.so\u001b[0m in \u001b[0;36mscipy.io.matlab.streams.FileStream.read_string (scipy/io/matlab/streams.c:4445)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: could not read bytes"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print key_name\n",
      "mat[key_name].dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test_segment_681\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "dtype([('data', 'O'), ('data_length_sec', 'O'), ('sampling_frequency', 'O'), ('channels', 'O')])"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(mat[key_name]['data'][0][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "16"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'filename' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-70-d7862759f93e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
       ]
      }
     ],
     "prompt_number": 70
    }
   ],
   "metadata": {}
  }
 ]
}